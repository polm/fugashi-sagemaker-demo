{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fugashi Word Count Tutorial\n",
    "\n",
    "In this tutorial we'll use fugashi, a wrapper for the MeCab tokenizer and morphological analysis tool, to count words in 吾輩は猫である \"I Am a Cat\", the famous novel by Natsume Soseki. \n",
    "\n",
    "MeCab uses large dictionaries and statistical models to tokenize Japanese text. Since the dictionaries are hosted on S3 they're easy to install in an AWS environment.\n",
    "\n",
    "First let's install the basic packages for this tutorial. \n",
    "\n",
    "- **fugashi** is the tool that actually performs tokenization\n",
    "- **unidic-lite** is the dictionary we'll use for this tutorial\n",
    "- **requests** will be used to download the book text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fugashi\n",
      "  Downloading fugashi-1.0.4-cp36-cp36m-manylinux1_x86_64.whl (476 kB)\n",
      "\u001b[K     |████████████████████████████████| 476 kB 15.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting unidic-lite\n",
      "  Downloading unidic-lite-1.0.7.tar.gz (47.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 47.3 MB 7.0 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests) (2.9)\n",
      "Building wheels for collected packages: unidic-lite\n",
      "  Building wheel for unidic-lite (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unidic-lite: filename=unidic_lite-1.0.7-py3-none-any.whl size=47556592 sha256=11e2858775e269091d6f7132659e2fbc6138a92d2b8c355db8efda3440ea293f\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/82/63/c6/b5f0ea5a04e01edc468cc78cd3d62deca919bbcb09116b37e6\n",
      "Successfully built unidic-lite\n",
      "Installing collected packages: fugashi, unidic-lite\n",
      "Successfully installed fugashi-1.0.4 unidic-lite-1.0.7\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install fugashi unidic-lite requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's do some basic tokenization. We'll print out a sentence along with the pronunciation and part-of-speech information for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩\tワガハイ\t代名詞,*,*,*\n",
      "は\tハ\t助詞,係助詞,*,*\n",
      "猫\tネコ\t名詞,普通名詞,一般,*\n",
      "で\tデ\t助動詞,*,*,*\n",
      "ある\tアル\t動詞,非自立可能,*,*\n",
      "。\t\t補助記号,句点,*,*\n"
     ]
    }
   ],
   "source": [
    "import fugashi\n",
    "tagger = fugashi.Tagger()\n",
    "for word in tagger(\"吾輩は猫である。\"):\n",
    "    print(word, word.feature.kana, word.pos, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UniDic part of speech tags have four parts, from basic part of speech type like 名詞 (noun) or 動詞 (verb) to more fine-grained tags, like whether a proper noun is a place or person. `*` is used as a placeholder when there's no detailed tag. \n",
    "\n",
    "UniDic includes a lot of other data beyond what we're using here, including kana accent, broad etymological category, and more. See XXX for details on the available fields.\n",
    "\n",
    "(Note: replace XXX with link to dataset details document.)\n",
    "\n",
    "Now let's download the book and try a basic word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words:\n",
      "9713 の\n",
      "9217 《\n",
      "9217 》\n",
      "7513 に\n",
      "7490 。\n",
      "7258 て\n",
      "6837 、\n",
      "6665 は\n",
      "6279 と\n",
      "6128 を\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# TODO put in S3 Bucket as sample document\n",
    "wagahai = requests.get(\"https://github.com/polm/ja-tokenizer-benchmark/raw/master/wagahai.txt\").text\n",
    "\n",
    "from collections import Counter\n",
    "wc = Counter()\n",
    "\n",
    "# Get a word count\n",
    "for line in wagahai.split(\"\\n\"):\n",
    "    for word in tagger(line):\n",
    "        wc[word.surface] += 1\n",
    "print(\"Most common words:\")        \n",
    "for key, val in wc.most_common(10):\n",
    "    print(val, key)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most common words are grammatical function words and punctuation, so looks like our word count is working. \n",
    "\n",
    "Now let's try getting a list of the most common proper nouns that are names of people. This should give us a list of characters in the novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common proper nouns:\n",
      "343 迷亭\n",
      "99 金田\n",
      "85 鈴木\n",
      "80 独仙\n",
      "42 雪江\n",
      "41 鼻子\n",
      "40 多々良\n",
      "36 りょう\n",
      "35 てい\n",
      "29 けん\n"
     ]
    }
   ],
   "source": [
    "wcpos = Counter()\n",
    "for line in wagahai.split(\"\\n\"):\n",
    "    for word in tagger(line):\n",
    "        if (word.feature.pos2 == '固有名詞' and\n",
    "            word.feature.pos3 == '人名'):\n",
    "            wcpos[word.surface] += 1\n",
    "\n",
    "print(\"Most common proper nouns:\")\n",
    "for key, val in wcpos.most_common(10):\n",
    "    print(val, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough the top entries are all names of characters in the novel. \n",
    "\n",
    "That's it for this basic tutorial; for more information see the documentation for [MeCab](https://taku910.github.io/mecab/) or [fugashi](https://github.com/polm/fugashi). happy tokenizing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
